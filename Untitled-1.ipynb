{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Initializing EEGComponent for subject 01 ====================\n",
      "\n",
      "Loading EEG data from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_sEEG.npy\n",
      "stimulus from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_stimuli.npy\n",
      "Channel info from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_channels.npy\n",
      "Cleaning EEG data...\n",
      "(307511, 130)\n",
      "EEG cleaning completed.\n",
      "Preprocessing EEG data...\n",
      "Performing electrode shaft referencing...\n",
      "(307511, 130)\n",
      "Electrode shaft referencing completed.\n",
      "Detrending and filtering EEG data...\n",
      "Applying bandpass and notch filters...\n",
      "EEG shape:  (307511, 130)\n",
      "EEG preprocessing completed.\n",
      "\n",
      "==================== Initializing AudioComponent for subject 01 ====================\n",
      "\n",
      "Loading and resampling audio from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_audio.npy\n",
      "Audio loaded and resampled to 16kHz. New shape: (6756812,)\n",
      "EEG data shape: (307511, 130)\n",
      "Audio data shape: (6756812,)\n",
      "\n",
      "==================== Initializing DataSegmenter ====================\n",
      "\n",
      "Segmenting data into chunks of size 0.05 seconds\n",
      "Mel spectrogram shape: (6007, 80)\n",
      "EEG segmented shape: (6029, 51, 130)\n",
      "Audio segments shape: (6006, 1125)\n",
      "----------------------------------------------------------------------\n",
      "Mel spectrogram shape: (6007, 80)\n",
      "EEG segmented shape: (6006, 51, 130)\n",
      "Audio segments shape: (6006, 1125)\n",
      "Segmented EEG data shape: (6006, 51, 130)\n",
      "Segmented mel data shape: (6007, 80)\n",
      "Segmented Audio data shape: (6006, 1125)\n",
      "\n",
      "==================== Initializing EEGAudioDataset ====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from src.components.data.data import EEGComponent, AudioComponent\n",
    "from src.components.data.data import DataSegmenter, AudioMelDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eeg_loader = EEGComponent(subject='01')\n",
    "audio_loader = AudioComponent(subject='01')\n",
    "\n",
    "eeg_data = eeg_loader._get_eeg()\n",
    "audio_data = audio_loader._get_audio()\n",
    "\n",
    "\n",
    "print(f\"EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Audio data shape: {audio_data.shape}\")\n",
    "\n",
    "segmenter = DataSegmenter(eeg_data, audio_data)\n",
    "eeg_segments, mel_spec_segments, audio_segments = segmenter.segment_data()\n",
    "\n",
    "print(f\"Segmented EEG data shape: {eeg_segments.shape}\")\n",
    "print(f\"Segmented mel data shape: {mel_spec_segments.shape}\")\n",
    "print(f\"Segmented Audio data shape: {audio_segments.shape}\")\n",
    "\n",
    "dataset = AudioMelDataset(\n",
    "    audio_segments=audio_segments, \n",
    "    mel_segments= mel_spec_segments\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hifigan.models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator, discriminator_loss, generator_loss, feature_loss\n",
    "from hifigan.env import AttrDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/home/owaismujtaba/work/NeuralSpeech/hifigan/config_v3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owaismujtaba/anaconda3/envs/neural/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "BATCH_SIZE = 8\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "EPOCHS = 50\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lambda_feat = 10.0  # Feature matching loss weight\n",
    "lambda_adv = 2.0    # Adversarial loss weight\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Load HiFi-GAN generator and discriminators\n",
    "# -----------------------------\n",
    "with open(config_file) as f:\n",
    "    config = AttrDict(json.load(f))\n",
    "\n",
    "generator = Generator(config).to(DEVICE)\n",
    "mpd = MultiPeriodDiscriminator().to(DEVICE)\n",
    "msd = MultiScaleDiscriminator().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owaismujtaba/anaconda3/envs/neural/lib/python3.10/site-packages/torch/nn/modules/loss.py:129: UserWarning: Using a target size (torch.Size([16, 1125])) that is different to the input size (torch.Size([1, 1125])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | G_Loss: 9.783678 | D_Loss: 3.964512\n",
      "Epoch 2/50 | G_Loss: 13.597456 | D_Loss: 3.822236\n",
      "Epoch 3/50 | G_Loss: 16.009096 | D_Loss: 3.775524\n",
      "Epoch 4/50 | G_Loss: 17.659993 | D_Loss: 3.749427\n",
      "Epoch 5/50 | G_Loss: 18.551587 | D_Loss: 3.736509\n",
      "Epoch 6/50 | G_Loss: 19.683606 | D_Loss: 3.716819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m loss_d_msd, _, _ \u001b[38;5;241m=\u001b[39m discriminator_loss(msd_real, msd_fake)\n\u001b[1;32m     56\u001b[0m loss_d \u001b[38;5;241m=\u001b[39m loss_d_mpd \u001b[38;5;241m+\u001b[39m loss_d_msd\n\u001b[0;32m---> 58\u001b[0m \u001b[43mloss_d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m optim_d\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m total_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_d\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/neural/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/neural/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/neural/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "generator.train()\n",
    "mpd.train()\n",
    "msd.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Optimizers & Losses\n",
    "# -----------------------------\n",
    "optim_g = torch.optim.Adam(generator.parameters(), lr=LR_G, betas=(0.8, 0.99))\n",
    "optim_d = torch.optim.Adam(list(mpd.parameters()) + list(msd.parameters()), lr=LR_D, betas=(0.8, 0.99))\n",
    "\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Training loop\n",
    "# -----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "\n",
    "    for audio, mel in loader:  # audio: [B, T], mel: [B, mel_channels, T]\n",
    "        mel = mel.T.to(DEVICE).float()\n",
    "        audio = audio.to(DEVICE).float()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator forward\n",
    "        # ---------------------\n",
    "        generated_audio = generator(mel)\n",
    "\n",
    "        # Trim to the same length\n",
    "        min_len = min(generated_audio.size(-1), audio.size(-1))\n",
    "        audio = audio[:, :min_len]\n",
    "        generated_audio = generated_audio[:, :min_len]\n",
    "\n",
    "        # Add channel dimension for discriminators\n",
    "        audio_in = audio.unsqueeze(1)              # [B, 1, T]\n",
    "        generated_in = generated_audio.unsqueeze(1)\n",
    "\n",
    "        # ---------------------\n",
    "        # Discriminator update\n",
    "        # ---------------------\n",
    "        optim_d.zero_grad()\n",
    "\n",
    "        mpd_real, mpd_fake, mpd_fmaps_r, mpd_fmaps_g = mpd(audio_in, generated_in.detach())\n",
    "        msd_real, msd_fake, msd_fmaps_r, msd_fmaps_g = msd(audio_in, generated_in.detach())\n",
    "\n",
    "        # Discriminator loss\n",
    "        loss_d_mpd, _, _ = discriminator_loss(mpd_real, mpd_fake)\n",
    "        loss_d_msd, _, _ = discriminator_loss(msd_real, msd_fake)\n",
    "        loss_d = loss_d_mpd + loss_d_msd\n",
    "\n",
    "        loss_d.backward()\n",
    "        optim_d.step()\n",
    "        total_d_loss += loss_d.item()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator update\n",
    "        # ---------------------\n",
    "        optim_g.zero_grad()\n",
    "\n",
    "        mpd_real, mpd_fake, mpd_fmaps_r, mpd_fmaps_g = mpd(audio_in, generated_in)\n",
    "        msd_real, msd_fake, msd_fmaps_r, msd_fmaps_g = msd(audio_in, generated_in)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_adv_mpd, _ = generator_loss(mpd_fake)\n",
    "        loss_adv_msd, _ = generator_loss(msd_fake)\n",
    "        loss_adv = loss_adv_mpd + loss_adv_msd\n",
    "\n",
    "        # Feature matching loss\n",
    "        loss_fm = feature_loss(mpd_fmaps_r, mpd_fmaps_g) + feature_loss(msd_fmaps_r, msd_fmaps_g)\n",
    "\n",
    "        # L1 waveform reconstruction\n",
    "        loss_l1 = l1_loss(generated_audio, audio)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_g = lambda_adv * loss_adv + lambda_feat * loss_fm + loss_l1\n",
    "        loss_g.backward()\n",
    "        optim_g.step()\n",
    "        total_g_loss += loss_g.item()\n",
    "\n",
    "    # ---------------------\n",
    "    # Epoch logging\n",
    "    # ---------------------\n",
    "    avg_g_loss = total_g_loss / len(loader)\n",
    "    avg_d_loss = total_d_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | G_Loss: {avg_g_loss:.6f} | D_Loss: {avg_d_loss:.6f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "torch.save({'generator': generator.state_dict()}, \"generator_finetuned_dutch.pt\")\n",
    "print(\"Generator saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owaismujtaba/anaconda3/envs/neural/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "import torch\n",
    "\n",
    "# Load HiFi-GAN vocoder\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# (Optionally) load a Dutch finetuned version if available\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\"your-hf-username/dutch-hifigan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5HifiGan(\n",
       "  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (upsampler): ModuleList(\n",
       "    (0): ConvTranspose1d(512, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (3): ConvTranspose1d(64, 32, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "  )\n",
       "  (resblocks): ModuleList(\n",
       "    (0): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (2): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (3): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (5): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (6): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (8): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (9): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (11): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocoder.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
