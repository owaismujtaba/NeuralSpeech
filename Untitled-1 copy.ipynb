{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Initializing EEGComponent for subject 01 ====================\n",
      "\n",
      "Loading EEG data from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_sEEG.npy\n",
      "stimulus from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_stimuli.npy\n",
      "Channel info from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_channels.npy\n",
      "Cleaning EEG data...\n",
      "(307511, 130)\n",
      "EEG cleaning completed.\n",
      "Preprocessing EEG data...\n",
      "Performing electrode shaft referencing...\n",
      "(307511, 130)\n",
      "Electrode shaft referencing completed.\n",
      "Detrending and filtering EEG data...\n",
      "Applying bandpass and notch filters...\n",
      "EEG shape:  (307511, 130)\n",
      "EEG preprocessing completed.\n",
      "\n",
      "==================== Initializing AudioComponent for subject 01 ====================\n",
      "\n",
      "Loading and resampling audio from /home/owaismujtaba/work/NeuralSpeech/data/raw/P01_audio.npy\n",
      "Audio loaded and resampled to 16kHz. New shape: (6621676,)\n",
      "EEG data shape: (307511, 130)\n",
      "Audio data shape: (6621676,)\n",
      "\n",
      "==================== Initializing DataSegmenter ====================\n",
      "\n",
      "Segmenting data into chunks of size 0.05 seconds\n",
      "Mel spectrogram shape: (6009, 80)\n",
      "EEG segmented shape: (6029, 51, 130)\n",
      "Audio segments shape: (6008, 1102)\n",
      "----------------------------------------------------------------------\n",
      "Mel spectrogram shape: (6009, 80)\n",
      "EEG segmented shape: (6008, 51, 130)\n",
      "Audio segments shape: (6008, 1102)\n",
      "Segmented EEG data shape: (6008, 51, 130)\n",
      "Segmented mel data shape: (6009, 80)\n",
      "Segmented Audio data shape: (6008, 1102)\n",
      "\n",
      "==================== Initializing EEGAudioDataset ====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from src.components.data.data import EEGComponent, AudioComponent\n",
    "from src.components.data.data import DataSegmenter, AudioMelDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eeg_loader = EEGComponent(subject='01')\n",
    "audio_loader = AudioComponent(subject='01')\n",
    "\n",
    "eeg_data = eeg_loader._get_eeg()\n",
    "audio_data = audio_loader._get_audio()\n",
    "\n",
    "\n",
    "print(f\"EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Audio data shape: {audio_data.shape}\")\n",
    "\n",
    "segmenter = DataSegmenter(eeg_data, audio_data)\n",
    "eeg_segments, mel_spec_segments, audio_segments = segmenter.segment_data()\n",
    "\n",
    "print(f\"Segmented EEG data shape: {eeg_segments.shape}\")\n",
    "print(f\"Segmented mel data shape: {mel_spec_segments.shape}\")\n",
    "print(f\"Segmented Audio data shape: {audio_segments.shape}\")\n",
    "\n",
    "dataset = AudioMelDataset(\n",
    "    audio_segments=audio_segments, \n",
    "    mel_segments= mel_spec_segments\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hifigan.models import Generator, MultiPeriodDiscriminator, MultiScaleDiscriminator, discriminator_loss, generator_loss, feature_loss\n",
    "from hifigan.env import AttrDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/home/owaismujtaba/work/NeuralSpeech/hifigan/pretrained/UNIVERSAL_V1/config.json'\n",
    "model_path = '/home/owaismujtaba/work/NeuralSpeech/hifigan/pretrained/UNIVERSAL_V1/do_02500000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "BATCH_SIZE = 8\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "EPOCHS = 50\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lambda_feat = 10.0  # Feature matching loss weight\n",
    "lambda_adv = 2.0    # Adversarial loss weight\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Load HiFi-GAN generator and discriminators\n",
    "# -----------------------------\n",
    "with open(config_file) as f:\n",
    "    config = AttrDict(json.load(f))\n",
    "\n",
    "generator = Generator(config).to(DEVICE)\n",
    "mpd = MultiPeriodDiscriminator().to(DEVICE)\n",
    "msd = MultiScaleDiscriminator().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_g = torch.load(model_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = state_dict_g['mpd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mstate_dict_g\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      8\u001b[0m mpd\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict_g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmpd\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m generator\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generator'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "mpd.load_state_dict(state_dict_g['mpd'])\n",
    "generator.train()\n",
    "mpd.train()\n",
    "msd.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Optimizers & Losses\n",
    "# -----------------------------\n",
    "optim_g = torch.optim.Adam(generator.parameters(), lr=LR_G, betas=(0.8, 0.99))\n",
    "optim_d = torch.optim.Adam(list(mpd.parameters()) + list(msd.parameters()), lr=LR_D, betas=(0.8, 0.99))\n",
    "\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Training loop\n",
    "# -----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "\n",
    "    for audio, mel in loader:  # audio: [B, T], mel: [B, mel_channels, T]\n",
    "        mel = mel.T.to(DEVICE).float()\n",
    "        audio = audio.to(DEVICE).float()\n",
    "        print(mel.shape, audio.shape)\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator forward\n",
    "        # ---------------------\n",
    "        generated_audio = generator(mel)\n",
    "\n",
    "        # Trim to the same length\n",
    "        min_len = min(generated_audio.size(-1), audio.size(-1))\n",
    "        audio = audio[:, :min_len]\n",
    "        generated_audio = generated_audio[:, :min_len]\n",
    "\n",
    "        # Add channel dimension for discriminators\n",
    "        audio_in = audio.unsqueeze(1)              # [B, 1, T]\n",
    "        generated_in = generated_audio.unsqueeze(1)\n",
    "\n",
    "        # ---------------------\n",
    "        # Discriminator update\n",
    "        # ---------------------\n",
    "        optim_d.zero_grad()\n",
    "\n",
    "        mpd_real, mpd_fake, mpd_fmaps_r, mpd_fmaps_g = mpd(audio_in, generated_in.detach())\n",
    "        msd_real, msd_fake, msd_fmaps_r, msd_fmaps_g = msd(audio_in, generated_in.detach())\n",
    "\n",
    "        # Discriminator loss\n",
    "        loss_d_mpd, _, _ = discriminator_loss(mpd_real, mpd_fake)\n",
    "        loss_d_msd, _, _ = discriminator_loss(msd_real, msd_fake)\n",
    "        loss_d = loss_d_mpd + loss_d_msd\n",
    "\n",
    "        loss_d.backward()\n",
    "        optim_d.step()\n",
    "        total_d_loss += loss_d.item()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator update\n",
    "        # ---------------------\n",
    "        optim_g.zero_grad()\n",
    "\n",
    "        mpd_real, mpd_fake, mpd_fmaps_r, mpd_fmaps_g = mpd(audio_in, generated_in)\n",
    "        msd_real, msd_fake, msd_fmaps_r, msd_fmaps_g = msd(audio_in, generated_in)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_adv_mpd, _ = generator_loss(mpd_fake)\n",
    "        loss_adv_msd, _ = generator_loss(msd_fake)\n",
    "        loss_adv = loss_adv_mpd + loss_adv_msd\n",
    "\n",
    "        # Feature matching loss\n",
    "        loss_fm = feature_loss(mpd_fmaps_r, mpd_fmaps_g) + feature_loss(msd_fmaps_r, msd_fmaps_g)\n",
    "\n",
    "        # L1 waveform reconstruction\n",
    "        loss_l1 = l1_loss(generated_audio, audio)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_g = lambda_adv * loss_adv + lambda_feat * loss_fm + loss_l1\n",
    "        loss_g.backward()\n",
    "        optim_g.step()\n",
    "        total_g_loss += loss_g.item()\n",
    "\n",
    "    # ---------------------\n",
    "    # Epoch logging\n",
    "    # ---------------------\n",
    "    avg_g_loss = total_g_loss / len(loader)\n",
    "    avg_d_loss = total_d_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | G_Loss: {avg_g_loss:.6f} | D_Loss: {avg_d_loss:.6f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "torch.save({'generator': generator.state_dict()}, \"generator_finetuned_dutch.pt\")\n",
    "print(\"Generator saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owaismujtaba/anaconda3/envs/neural/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "import torch\n",
    "\n",
    "# Load HiFi-GAN vocoder\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# (Optionally) load a Dutch finetuned version if available\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\"your-hf-username/dutch-hifigan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechT5HifiGan(\n",
       "  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (upsampler): ModuleList(\n",
       "    (0): ConvTranspose1d(512, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "    (3): ConvTranspose1d(64, 32, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "  )\n",
       "  (resblocks): ModuleList(\n",
       "    (0): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (2): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (3): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (5): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (6): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (8): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (9): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (11): HifiGanResidualBlock(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocoder.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
